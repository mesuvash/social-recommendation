Make sure each chapter has intro/conclusion


Interactions more predictive then messages, messages are a much weaker predictive interaction.

This is a sufficiently large dataset for performing our analysis.

We conclude by summarising the key novel observations found during this research.

Some of these online interactions suggest a real world relationship such as being tagged in photos or videos.

Only include items that have had at least k likes among the members of alters. (Romero, Meeder and Kleinberg)
Romero2011hashtag

Smaller groups are generally more predictive which adheres to the common sense notion that large groups appeal more 
broadly (www)

Multiple studies have found that online interactions tend to corelate more with interests then user profile (www)

Anderson et al 2012 concluded that it is less about the content of what people say, but more about who they interacted with.

%%
%% Template conclusion.tex
%%

\chapter{Feature Combination}
\label{cha:bma}

Given the vast number of affinity features outlined in the previous sections, it is both computationally costly and time 
consuming to group all features together into one combined feature. Hence, it is crucial we provide a practical method which 
facilitates the combination of the most individually predictive affinity features found during this research.

In this chapter we combine these individually predictive affinity features together and examine the results.

\section{Affinity Feature Selection}
\label{sec:notation}

Based on results found during our \emph{user interactions} and \emph{user preference} analysis, the most predictive features we have found are:
\begin{itemize}
\item \textbf{Favourites}
\item \textbf{Groups}
\item \textbf{Pages}
\end{itemize}

The following results and analysis are based on these above features combined.

\clearpage

Applying this combined affinity feature to the data set we obtain:

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/bar_combination.pdf}
		\caption{Accuracy results using the \emph{combined} feature set. This \emph{combined} feature gives our best prediction 
		in comparison to all other individual features and we can see a large improvement over our SMB baseline for the case where $k=0$, particularly for the LR case.}
	\end{center}
\end{figure}

\clearpage

We find that the \emph{combined} affinity feature gives the most predictive results when compared with both our baselines and all
other individual features tested during this research.

The most predictive individual results of \emph{favourites}, \emph{groups} and \emph{pages} against the \emph{combined}
feature are shown below:

\begin{table}[h]
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.583 $\pm$ 0.012 \\ \hline
		LR & 0.617 $\pm$ 0.01 \\ \hline
		SVM & 0.614 $\pm$ 0.009 \\ \hline
  \end{tabular}
  \caption{\emph{Favourite} feature results for $k=0$.}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.604 $\pm$ 0.01 \\ \hline
		LR & 0.613 $\pm$ 0.01 \\ \hline
		SVM & 0.611 $\pm$ 0.008 \\ \hline
  \end{tabular}
  \caption{\emph{Pages} feature results for $k=0$.}
\end{minipage}
\\
\\
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.565 $\pm$ 0.013 \\ \hline
		LR & 0.622 $\pm$ 0.008 \\ \hline
		SVM & 0.609 $\pm$ 0.011 \\ \hline
  \end{tabular}
  \caption{\emph{Groups} feature results for $k=0$.}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.605 $\pm$ 0.01 \\ \hline
		LR & 0.624 $\pm$ 0.009 \\ \hline
		SVM & 0.618 $\pm$ 0.01 \\ \hline
  \end{tabular}
  \caption{\emph{Combined} feature results for $k=0$.}
\end{minipage}
\end{table}

These tables clearly show that the most predictive feature vector is a combination of the individually most predictive 
features found during our analysis. This trend continues across our exposure $k$ and offers the most predictive feature 
vector found during this research over all exposures.

This graph can be seen below:

\clearpage

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/line_combination.pdf}
		\caption{Accuracy results across exposure using the \emph{combined} feature set. This feature set offers the most predictive 
				 results discovered during this research for all $k$. As our exposure increases, the improved predictive performance of this 
				 \emph{combined} feature increases too.}
	\end{center}
\end{figure}

\clearpage

By extracting the model feature weights from the case where $k=0$ we can see which components of the \emph{combined} affinity feature 
were most predictive:

\begin{table}[h]
\begin{minipage}[b]{1.0\textwidth}
\centering
  \begin{tabular}{|l|l|l|l|l|} % cols: (left, center, right)
  \hline
  \textbf{Name} & \textbf{Size} & \textbf{Weight} & \textbf{Frequency} \\ \hline

\small{Avatar: The Last Airbender (Page)} & 324 & -1.68 $\pm$ 0.001 & 13 \\ \hline
\small{I'm late. Got attacked by a wild Pokemon (Page)} & 161 & -1.609 $\pm$ 0 & 20 \\ \hline
\small{Overheard at the Ateneo de Manila (Group)} & 253 & -1.527 $\pm$ 0.001 & 26 \\ \hline
\small{Sorry mate i can't, i've got Quidditch (Page)} & 254 & -1.501 $\pm$ 0 & 18 \\ \hline
\small{I would.........for Escapium. (Group)} & 50 & -1.467 $\pm$ 0.001 & 11 \\ \hline
\small{Burgtoons (Group)} & 34 & -1.37 $\pm$ 0.001 & 7 \\ \hline
\small{The Simpsons (Page)} & 1552 & -1.355 $\pm$ 0.001 & 170 \\ \hline
\small{City Gate Hall (Group)} & 27 & -1.346 $\pm$ 0 & 5 \\ \hline
\small{Victoria's Secret (Page)} & 764 & -1.337 $\pm$ 0 & 11 \\ \hline
\small{Starbucks (Page)} & 1548 & -1.313 $\pm$ 0 & 7 \\ \hline
  \end{tabular}
  \caption{LR feature weights extracted for the negative case where $k=0$. The \emph{Name} column displays the name of the feature.
                        \emph{Size} represents the size of the \emph{Page} or \emph{Group}.
                        \emph{Weight} represents the weight this feature vector received.  
                        \emph{Frequency} displays the number of times this feature vector was set to $1$ for a user.}
\end{minipage}
\end{table}
\begin{table}[h]
\begin{minipage}[b]{1.0\textwidth}
\centering
  \begin{tabular}{|l|l|l|l|l|} % cols: (left, center, right)
  \hline
  \textbf{Name} & \textbf{Size} & \textbf{Weight} & \textbf{Frequency} \\ \hline

\small{Don't you hate it when Gandalf marks [...] (Page)} & 110 & 1.627 $\pm$ 0.001 & 19 \\ \hline
\small{Goodberry's (Page)} & 318 & 1.591 $\pm$ 0 & 73 \\ \hline
\small{Worst. Idea. Ever. [pause] Let's do it. (Page)} & 227 & 1.561 $\pm$ 0 & 21 \\ \hline
\small{CatDog (Page)} & 259 & 1.531 $\pm$ 0.001 & 12 \\ \hline
\small{Planking Australia (Page)} & 166 & 1.501 $\pm$ 0.001 & 4 \\ \hline
\small{Avenged Sevenfold (Page)} & 351 & 1.471 $\pm$ 0 & 6 \\ \hline
\small{Grug (Page)} & 279 & 1.465 $\pm$ 0 & 9 \\ \hline
\small{Dr. House (Page)} & 964 & 1.451 $\pm$ 0 & 28 \\ \hline
\small{If 1m people join, girlfriend will let [...] (Group)} & 416 & 1.362 $\pm$ 0 & 68 \\ \hline
\small{Do you ride kangaroos? no mate the [...] (Page)} & 321 & 1.333 $\pm$ 0.001 & 23 \\ \hline
  \end{tabular}
  \caption{LR feature weights extracted for the positive case where $k=0$. The \emph{Name} column displays the name of the feature.
                        \emph{Size} represents the size of the \emph{Page} or \emph{Group}.
                        \emph{Weight} represents the weight this feature vector received.  
                        \emph{Frequency} displays the number of times this feature vector was set to $1$ for a user.}
\end{minipage}
\end{table}

The negative LR weights are equally broken up into \emph{pages} and \emph{groups} as contributing highly to the prediction. The size ranges 
for both \emph{pages} and \emph{groups} varies greatly from as little as $27$ up to as high as $1552$ and there is little corelation between 
the frequency and sizes, in fact lower frequencies of around $20$ contribute most to the prediction.

The positive LR weights are more focused on the predictiveness of \emph{pages} and the sizes of the \emph{pages} are in
a much more consistent range, while the frequencies still vary.

These LR weights show that both \emph{groups} and especially \emph{pages} are highly predictive of a users like preferences and particular 
sizes or frequencies do not appear to be more predictive then others. Additionally under this \emph{combined} paradigm, \emph{favourites} do not appear 
to contribute strongly to our results.

We have shown in this chapter that the \emph{combined} feature of our individually most predictive affinity features results in the most 
accurate and concise predictions for our data set, when \emph{combined}, \emph{favourites} are less predictive then \emph{groups} and 
\emph{groups} are less predictive then \emph{pages}. Additionally, the sizes and frequencies of these \emph{groups} and \emph{pages} 
do not appear to be consistent in their predictive qualities.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
