%%
%% Template conclusion.tex
%%

\chapter{Feature Combination}
\label{cha:bma}

Given the vast number of affinity features outlined in the previous sections and the scope of users and associated data on Facebook, it is computationally costly, time 
consuming and ineffective to naively combine all features together. Hence, it is crucial we provide a practical approach which 
facilitates the combination of the individually most predictive affinity features found during this research from which we can efficiently learn.

In this chapter we combine the individually most predictive affinity features together and examine the results.

\section{Affinity Feature Selection}
\label{sec:notation}

Based on results found during our \emph{user interactions} and \emph{user preference} analysis, the most predictive features we have found were:
\begin{itemize}
\item \textbf{Favourites}
\item \textbf{Groups}
\item \textbf{Pages}
\end{itemize}

The following results and analysis are based on these above features combined.

\clearpage

Applying this combined affinity feature to the data set we obtain:

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/bar_combination.pdf}
		\caption{Accuracy results using the \emph{combined} feature set. This \emph{combined} features give our best prediction 
		in comparison to all other individual features and we see a large improvement over our SMB baseline, particularly for the LR case.}
	\end{center}
\end{figure}

\clearpage

We find that the \emph{combined} affinity feature gives the most predictive results when compared with both our baselines and all
other individual features tested during this research.

The most predictive individual results of \emph{favourites}, \emph{groups} and \emph{pages} against the \emph{combined}
feature are tabulated below:

\begin{table}[h]
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.583 $\pm$ 0.012 \\ \hline
		LR & 0.617 $\pm$ 0.01 \\ \hline
		SVM & 0.614 $\pm$ 0.009 \\ \hline
  \end{tabular}
  \caption{\emph{Favourite} feature results for $k=0$.}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.604 $\pm$ 0.01 \\ \hline
		LR & 0.613 $\pm$ 0.01 \\ \hline
		SVM & 0.611 $\pm$ 0.008 \\ \hline
  \end{tabular}
  \caption{\emph{Pages} feature results for $k=0$.}
\end{minipage}
\\
\\
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.565 $\pm$ 0.013 \\ \hline
		LR & 0.622 $\pm$ 0.008 \\ \hline
		SVM & 0.609 $\pm$ 0.011 \\ \hline
  \end{tabular}
  \caption{\emph{Groups} feature results for $k=0$.}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.605 $\pm$ 0.01 \\ \hline
		LR & 0.624 $\pm$ 0.009 \\ \hline
		SVM & 0.618 $\pm$ 0.01 \\ \hline
  \end{tabular}
  \caption{\emph{Combined} feature results for $k=0$.}
\end{minipage}
\end{table}

These tables clearly show that the most predictive feature vector is a combination of the individually most predictive 
features found during our analysis. This trend continues across our exposure $k$ and this \emph{combined} feature offers the most predictive results 
found during this research over all exposures.

This can be seen in the graph below:

\clearpage

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/line_combination.pdf}
		\caption{Accuracy results across exposure using the \emph{combined} feature set. This feature offers the most predictive 
				 results discovered during this research for all $k$. As our exposure increases, the improved predictive performance of this 
				 \emph{combined} feature increases too.}
	\end{center}
\end{figure}

\clearpage

By extracting the model feature weights from the case where $k=0$ we can see which \emph{combined} components 
were most predictive:

\begin{table}[h]
\begin{minipage}[b]{1.0\textwidth}
\centering
  \begin{tabular}{|l|l|l|l|l|} % cols: (left, center, right)
  \hline
  \textbf{Name} & \textbf{Size} & \textbf{Weight} & \textbf{Frequency} \\ \hline

\small{\textbf{(Page)} Avatar: The Last Airbender} & 324 & 1.68 $\pm$ 0.001 & 13 \\ \hline
\small{\textbf{(Page)} I'm late. Got attacked by a wild Pokemon} & 161 & 1.609 $\pm$ 0 & 20 \\ \hline
\small{\textbf{(Group)} Overheard at the Ateneo de Manila} & 253 & 1.527 $\pm$ 0.001 & 26 \\ \hline
\small{\textbf{(Page)} Sorry mate i can't, i've got Quidditch} & 254 & 1.501 $\pm$ 0 & 18 \\ \hline
\small{\textbf{(Group)} I would.........for Escapium.} & 50 & 1.467 $\pm$ 0.001 & 11 \\ \hline
\small{\textbf{(Group)} Burgtoons} & 34 & 1.37 $\pm$ 0.001 & 7 \\ \hline
\small{\textbf{(Page)} The Simpsons} & 1552 & 1.355 $\pm$ 0.001 & 170 \\ \hline
\small{\textbf{(Group)} City Gate Hall} & 27 & 1.346 $\pm$ 0 & 5 \\ \hline
\small{\textbf{(Page)} Victoria's Secret} & 764 & 1.337 $\pm$ 0 & 11 \\ \hline
\small{\textbf{(Page)} Starbucks} & 1548 & 1.313 $\pm$ 0 & 7 \\ \hline
  \end{tabular}
  \caption{LR feature weights extracted for the positive case. The \emph{Name} column displays the name of the feature.
                        \emph{Size} represents the size of the \emph{Page} or \emph{Group}.
                        \emph{Weight} represents the weight this feature vector received.  
                        \emph{Frequency} displays the number of times this feature vector was set to $1$ for a user.}
\end{minipage}
\end{table}
\begin{table}[h]
\begin{minipage}[b]{1.0\textwidth}
\centering
  \begin{tabular}{|l|l|l|l|l|} % cols: (left, center, right)
  \hline
  \textbf{Name} & \textbf{Size} & \textbf{Weight} & \textbf{Frequency} \\ \hline

\small{\textbf{(Page)} Don't you hate it when Gandalf marks [...] } & 110 & -1.627 $\pm$ 0.001 & 19 \\ \hline
\small{\textbf{(Page)} Goodberry's } & 318 & -1.591 $\pm$ 0 & 73 \\ \hline
\small{\textbf{(Page)} Worst. Idea. Ever. [pause] Let's do it. } & 227 & -1.561 $\pm$ 0 & 21 \\ \hline
\small{\textbf{(Page)} CatDog } & 259 & -1.531 $\pm$ 0.001 & 12 \\ \hline
\small{\textbf{(Page)} Planking Australia } & 166 & -1.501 $\pm$ 0.001 & 4 \\ \hline
\small{\textbf{(Page)} Avenged Sevenfold } & 351 & -1.471 $\pm$ 0 & 6 \\ \hline
\small{\textbf{(Page)} Grug } & 279 & -1.465 $\pm$ 0 & 9 \\ \hline
\small{\textbf{(Page)} Dr. House } & 964 & -1.451 $\pm$ 0 & 28 \\ \hline
\small{\textbf{(Group)} If 1m people join, girlfriend will let [...] } & 416 & -1.362 $\pm$ 0 & 68 \\ \hline
\small{\textbf{(Page)} Do you ride kangaroos? no mate the [...] } & 321 & -1.333 $\pm$ 0.001 & 23 \\ \hline
  \end{tabular}
  \caption{LR feature weights extracted for the negative case. The \emph{Name} column displays the name of the feature.
                        \emph{Size} represents the size of the \emph{Page} or \emph{Group}.
                        \emph{Weight} represents the weight this feature vector received.  
                        \emph{Frequency} displays the number of times this feature vector was set to $1$ for a user.}
\end{minipage}
\end{table}

The positive LR weights are equally broken up into \emph{pages} and \emph{groups} as contributing highly to a prediction of like. The size ranges 
for both \emph{pages} and \emph{groups} varies greatly from as little as $27$ up to as high as $1552$ and there is little corelation between 
the frequency and sizes, in fact lower frequencies of around $20$ contribute most to the like prediction.

The negative LR weights are more focused on the predictiveness of \emph{pages} for a like and the sizes of the \emph{pages} are in
a much more consistent range, while the frequencies still vary.

These LR weights show that both \emph{groups} and especially \emph{pages} are highly predictive of a users like preferences and particular 
sizes or frequencies do not appear to be more predictive then others. Both \emph{groups} and \emph{pages} are indicitve of a dislike prediction 
while \emph{pages} are most predictive for a like. Additionally under this \emph{combined} paradigm, \emph{favourites} do not appear 
to contribute strongly to our results.

We have shown in this chapter that the \emph{combined} feature of our individually most predictive features results in the most 
accurate and concise predictions from our data set, when combined \emph{favourites} are less predictive then \emph{groups} and 
\emph{groups} are less predictive then \emph{pages}. \emph{Pages} and \emph{groups} both contribute to a dislike prediction, while a like
prediction is more strongly associated with \emph{pages} - which is not surprising as \emph{pages} are more predictive then \emph{groups}.
Additionally, the sizes and frequencies of these \emph{groups} and \emph{pages} do not appear to be consistent in their predictive qualities.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
