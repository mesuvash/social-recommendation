%%
%% Template intro.tex
%%

\chapter{Background}
\label{cha:back}

In the following section, we define the source of our data set, notation used throughout this thesis, our choice of classification algorithms and 
our testing approach and methodology.

\section{Facebook}
\label{sec:data}

Facebook is the largest and most active social media service in the world. Facebook users can create a profile containing personal 
preferences and information (age, birthday, group preferences, favourite athlete, etc) and have friendships and interactions 
between other users. The four main interactions between users are posts (posting an element on a friends' wall), tags
(being mentioned in a friends post or comment), comments (written data on a post) and likes (clicking a like button if a user 
likes a post or comment). The three mediums for these interactions are across links (some URL), posts (some Facebook post), 
photos (some uploaded Facebook photo) and videos (some uploaded Facebook video).

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.85]{results/imageLiked.png}
		\caption{Here we see an example of a link posted to a friends wall, which has subsequently been liked by two friends.}
	\end{center}
\end{figure}

\clearpage
One issue present in this Facebook paradigm is discovering whether a user doesn't like an item, a users Facebook feed is comprised of 
activity between their friends, content, groups, etc given the enormous scope of potential feed items, Facebook will only show feed 
items for users who you have recently interacted with using their \emph{Edge-Rank} ~\cite{edge} algorithm. 

While many Facebook users have a friend count which is close to the human real word limit, known as the Dunbar number 
~\cite{hill2003social}, the \emph{Edge-Rank} algorithm ensures user interactions are focused on a much smaller subset of their friends.
Additionally, given the rate of posting, these top feed items are only displayed for a short amount of time. This is coupled 
with the fact that Facebook allows users to explicitly like an item, but not dislike it - hence distinguishing between what 
a user does and does not like becomes difficult.

Given this issue, NICTA have developed an app which explicitly determines a users preference for an item, by facilitating explicit 
like and dislike options, which will be discussed in the following section.

%\cite{backstrom2011center} studied two types of user uses of Facebook, explicit communication interaction and viewing attention. Communication 
%is focused on a limited subset of friends whilst viewing attention is dispersed among a much larger set. This supports the approach of testing
%a wide array of user interactiuons and preferences, as each users preferences are driven by where their attention is focused.

\section{LinkR}
\label{sec:linkr}

NICTA developed a Facebook app named \emph{LinkR} \footnote{The main developer of the LinkR Facebook App is Khoi-Nguyen Tran, 
a PhD student at the Australian National University.} which would make recommendations to users and record whether or not the 
user liked the recommended item.

The app also collected information about users, their interactions and preferences as well as a subset of available information about 
their friends. The app tracked and stored information for over 100 app users and their 39,000+ friends over a 4-month time period.

The table below summarises the data collected from both app users and their friends.

\begin{table}[!htbp]
\centering
	\begin{tabular}{|l|r|r|r|r|} % cols: (left, center, right)
		\hline
		\textbf{App Users} & \textbf{Posts} & \textbf{Tags} & \textbf{Comments} & \textbf{Likes}  \\ \hline
		\textbf{Wall} & 27,955 & 5,256 & 15,121 & 11,033 \\ \hline
		\textbf{Link} & 3,974 & - & 5,757 & 4,279 \\ \hline
		\textbf{Photo} & 4,147 & 22,633 & 8,677 & 5,938 \\ \hline
		\textbf{Video} & 211 & 2,105 & 1,687 & 710 \\ \hline
		 \hline
		\textbf{App Users and Friends} & \textbf{Posts} & \textbf{Tags} & \textbf{Comments} & \textbf{Likes}  \\ \hline
		\textbf{Wall} & 3,384,740 & 912,687 & 2,152,321 & 1,555,225 \\ \hline
		\textbf{Link} & 514,475 & - & 693,930 & 666,631 \\ \hline
		\textbf{Photo} & 1,098,679 & 8,407,822 & 2,978,635 & 1,960,138 \\ \hline
		\textbf{Video} & 56,241 & 858,054 & 463,401 & 308,763 \\ \hline
	\end{tabular}
	\caption{Data records for interactions between users. Rows are the type of interaction, columns are the medium.}
	\label{tab:revpol}
\end{table}

\clearpage

Pertinent user features we will exploit during this thesis include:
\begin{itemize}
\item Gender
\item Age
\item Hometown
\item Locale
\item Group Memberships
\item Page Likes
\item Favourite Activities
\item Favourite Books
\item Favourite Athletes
\item Favourite Teams
\item Inspirational People
\item Interests
\item Favourite Movies
\item Favourite Music
\item Favourite Sports
\item Favourite Television Shows
\item School Information
\item Work Information
\item Messages data
\end{itemize}

\section{Notation}
\label{sec:notation}

The mathematical notation used by our classifiers during this thesis are outlined below.

\begin{itemize}
\item A set of users of size $N$. 
\item A set of items of size $M$.
\item A user feature vector $X$ of size $i$, the size of each feature vector varies based on the current user features being analysed
and is explicitly defined in each section. In other words $X = \langle x_1, x_2, \dots , x_i \rangle$
\item A set of alters $A$ of size $j$, the size and composition of each alters set varied based on the current user features being 
analysed and is explicitly defined in each section.
\item An exposure of size $k$, where each $k$ represents the number of some user $n$'s friends who have liked some item $m$.
\item A data-set $D$ comprised of $D = \{(n,m,x_i) \to y\}$ with the binary response $y \in \{0,1\}$ where $0$ represents a dislike 
and $1$ represents a like.
\end{itemize}

\section{Feature Vectors}
\label{sec:features}

Individual feature vectors for $X$ will be discussed and defined in their appropriate sections under \emph{User Interactions} and 
\emph{User Preferences}. During this thesis we will analyse the results gained from the $7$ different user features listed below, 
additionally a combination of the most predictive individual feature vectors will also be tested.

\begin{itemize}
\item Interactions
\item Demographics
\item Traits
\item Groups
\item Pages
\item Outgoing Messages
\item Incoming Messages
\end{itemize}

\section{Previous Work}
\label{sec:pw}

Below we discuss previous work done in this classification area completed in 2011 by Joseph Noel.

\subsection{Social Collaborative Filtering}
\label{sec:cbf}

Two general approaches to classification prediction are \emph{content-based filtering} (CBF) ~\cite{newsweeder} which exploits 
item features based on items a user has previously liked, or the second approach which is \emph{collaborative filtering} (CF) 
~\cite{collab_filtering} which exploits the current user's preferences as well as those of other users.

Previous work defined the term \emph{social} CF (SCF) ~\cite{joseph} which augments traditional CF methods with additional social 
network information, the results of this previous work and analysis came to the conclusion that the approach of \emph{Social Matchbox}
provided the best results for this data set. In live user trials SMB provided the best performance against all other implemented 
algorithms and as such will be used as a baseline in this thesis.

\section{Training and Testing}
\label{sec:tt}

All evaluation is done using 10 fold cross validation wherein the data is partitioned into 10 complimentary subsets, each 
subset is composed of two separate parts, one part is used for training $(80\%)$ and the other $(20\%)$ is used for testing. 
All training and testing is performed on each distinct fold and the results are averaged along with their standard error in each
table and graph.

\section{Classification Algorithms}
\label{sec:meth}

Each classification algorithm used in this thesis is passed the training set for each fold as discussed previously. Based on this 
data and the current feature vector $X$ the classifier builds a model representation of the data and uses this model to classify 
each component in the test set into either a like or a dislike.

All feature vector analysis carried out in this thesis will be performed on the following classification algorithms.

\subsection{Constant}
\label{sec:const}

The constant (Constant) predictor returns a constant result irrespective of the feature vectors selected. The most common result 
in our data set is $False$ and hence the $False$ predictor is displayed in all analysis, tables and graphs.

\subsection{Social Match Box}
\label{sec:sr}

SMB is an extension of existing SCF techniques~\cite{lla,socinf} which constrain the latent space to enforce users 
who have similar preferences to maintain similar latent representations when they interact heavily.

SMB uses the \emph{Social Regularization} method which incorporates user features to learn
similarities between users in the latent space which allows us to incorporate the social information of the Facebook data ~\cite{joseph}.
This objective component constrains users with a high similarity rating to have the same values in the latent feature space, which
models the assumption that users who are similar socially should also have similar preferences for items.

\subsection{Naive Bayes}
\label{sec:nb}

\emph{Naive Bayes} is a basic probabilistic classifier which involves applying Bayes' theorem using strong conditional independence 
assumptions between each feature in $X$. During training each element $i$ in the feature vector is devised to contribute some 
evidence that this $X$ belongs to either a like or dislike classification, during testing the class with the highest probability 
when applied to the model is the classification returned. 

The NB implementation used during this thesis is an implementation previously devised by Scott Sanner.
\footnote{Scott Sanner is a Senior Researcher in the Machine Learning Group at NICTA and the supervisor for this research.}.

\subsection{Logistic Regression}
\label{sec:lr}

\emph{Logistic Regression} directly estimates parameters based on the training data assuming a parametric form of the distribution.
LR predicts the odds of a feature vector $X$ being either a like or a dislike by converting a dependent variable and 
one or more continuous independent variable(s) into probability odds.

The LR implementation used during this thesis is \emph{LingPipe} \cite{lin}.

\subsection{Support Vector Machine}
\label{sec:svm}

The \emph{Support Vector Machine} is a supervised learning machine based on a set of basis functions which help construct 
a separating hyperplane between data points. Training involves building the relevant hyperplanes which can then be used for testing. 
Each data point is classified as a like or dislike depending on which side of the hyperplane it falls.

The SVM implementation used during this thesis is \emph{SVMLibLinear} \cite{cjlin}.

\section{Evaluation Metrics}
\label{sec:notation}

When evaluating the success of each feature vector at correctly classifying an item, the following metrics will be analysed.

\begin{itemize}
\item A \emph{true positive} (TP) prediction refers to when the classifier correctly identifies the class as true. 
\item A \emph{false positive} (FP) occurs when the prediction is true, but the true class was false. 
\item A \emph{false negative} (FN) occurs when the prediction is false but the actual class is true.
\end{itemize}

Accuracy relates to the closeness to the true value. In the context of our results, the accuracy refers to the number of correct classifications 
divided by the size of the data set.

\[ \text{accuracy} = \frac{\text{number of correct classifications}}{\text{size of the test data set}}\]

Precision relates to the number of retrieved predictions which are relevant. In the context of our results, the precision refers to the number of TP predictions 
divided by the sum of the TP and FP predictions.

\[ \text{precision} = \frac{\text{number of TP}}{\text{number of TP + number of FP}}\]

Recall refers to the number of relevant predictions that are retrieved. In the context of our results, recall refers to the number of TP predictions 
divided by the sum of the TP and FN predictions.

\[ \text{recall} = \frac{\text{number of TP}}{\text{number of TP + number of FN}}\]

The f-score combines and balances both precision and recall and is interpreted as the weighted average of both precision and recall. 

\[ \text{f-score} = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}\]

The main metric we use for analysis, tabulation and graphing in this thesis is accuracy.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
