%%
%% Template conclusion.tex
%%

\chapter{Feature Combination}
\label{cha:bma}

Given the vast number of potential affinity features as outlined in the previous sections, it is both computationally costly and time 
consuming to group all features together into one huge combined affinity feature. Hence, it is crucial we provide a practical method which 
facilitates a combination of the most predictive individual affinity features found during this research.

In this chapter we combine the individually most predictive affinity features together into one large, but manageable feature vector 
and examine the results.

\section{Affinity Feature Selection}
\label{sec:notation}

Based on results found during our \emph{user preference} analysis, the combined affinity feature we examine is comprised of:
\begin{itemize}
\item \emph{Favourites}
\item \emph{Groups}
\item \emph{Pages}
\end{itemize}

\clearpage

Applying this combined affinity feature to the data set:

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/bar_combination.pdf}
		\caption{Accuracy results using the \emph{combined} feature set. We can see a large improvement over our SMB baseline for the case
				 where $k=0$, particularly for the LR case.}
	\end{center}
\end{figure}

\clearpage

We find that the \emph{combined} affinity feature gives better results for our classifiers when compared with our baselines, particularly for 
the LR case. In fact, this \emph{combined} affinity feature results in the best results found during this research, which is summarised in 
the table below:

\begin{table}[h]
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.583 $\pm$ 0.012 \\ \hline
		LR & 0.617 $\pm$ 0.01 \\ \hline
		SVM & 0.614 $\pm$ 0.009 \\ \hline
  \end{tabular}
  \caption{\emph{Favourite} feature results for $k=0$.}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.604 $\pm$ 0.01 \\ \hline
		LR & 0.613 $\pm$ 0.01 \\ \hline
		SVM & 0.611 $\pm$ 0.008 \\ \hline
  \end{tabular}
  \caption{\emph{Pages} feature results for $k=0$.}
\end{minipage}
\\
\\
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.565 $\pm$ 0.013 \\ \hline
		LR & 0.622 $\pm$ 0.008 \\ \hline
		SVM & 0.609 $\pm$ 0.011 \\ \hline
  \end{tabular}
  \caption{\emph{Groups} feature results for $k=0$.}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.605 $\pm$ 0.01 \\ \hline
		LR & 0.624 $\pm$ 0.009 \\ \hline
		SVM & 0.618 $\pm$ 0.01 \\ \hline
  \end{tabular}
  \caption{\emph{Combined} feature results for $k=0$.}
\end{minipage}
\end{table}

These results show that based on our results the most predictive feature vector is a combination of the individual most predictive 
feature vectors found in our \emph{user preferences} section.

This trend continues for all values of $k$ and offers the most predictive feature vector found during this research.

\clearpage

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/line_combination.pdf}
		\caption{Accuracy results across exposure using the \emph{combined} feature set. This feature set offers the most predictive 
				 results discovered during this research. As our exposure $k$ increases, the improved predictive performance of this 
				 feature combination increases with it.}
	\end{center}
\end{figure}

Clearly, this \emph{combined} feature provides the most predictive results when compared with all other analysis completed 
during this thesis.

\clearpage

By extracting the model feature weights from the case where $k=0$ we can see which components of the \emph{combined} affinity feature 
were most predictive:

\begin{table}[h]
\begin{minipage}[b]{1.0\textwidth}
\centering
  \begin{tabular}{|l|l|l|l|l|} % cols: (left, center, right)
  \hline
  \textbf{Name} & \textbf{Size} & \textbf{Weight} & \textbf{Frequency} \\ \hline

\small{Avatar: The Last Airbender (Page)} & 324 & -1.68 $\pm$ 0.001 & 13 \\ \hline
\small{I'm late. Got attacked by a wild Pokemon (Page)} & 161 & -1.609 $\pm$ 0 & 20 \\ \hline
\small{Overheard at the Ateneo de Manila (Group)} & 253 & -1.527 $\pm$ 0.001 & 26 \\ \hline
\small{Sorry mate i can't, i've got Quidditch (Page)} & 254 & -1.501 $\pm$ 0 & 18 \\ \hline
\small{I would.........for Escapium. (Group)} & 50 & -1.467 $\pm$ 0.001 & 11 \\ \hline
\small{Burgtoons (Group)} & 34 & -1.37 $\pm$ 0.001 & 7 \\ \hline
\small{The Simpsons (Page)} & 1552 & -1.355 $\pm$ 0.001 & 170 \\ \hline
\small{City Gate Hall (Group)} & 27 & -1.346 $\pm$ 0 & 5 \\ \hline
\small{Victoria's Secret (Page)} & 764 & -1.337 $\pm$ 0 & 11 \\ \hline
\small{Starbucks (Page)} & 1548 & -1.313 $\pm$ 0 & 7 \\ \hline
  \end{tabular}
  \caption{\emph{Logistic Regression} feature weights extracted for the negative case where $k=0$. The \emph{Name} column displays the name of the feature.
                        \emph{Size} represents the size of the \emph{Page} or \emph{Group}.
                        \emph{Weight} represents the weight this feature vector received.  
                        \emph{Frequency} displays the number of times this feature vector was set to $1$ for a user.}
\end{minipage}
\end{table}
\begin{table}[h]
\begin{minipage}[b]{1.0\textwidth}
\centering
  \begin{tabular}{|l|l|l|l|l|} % cols: (left, center, right)
  \hline
  \textbf{Name} & \textbf{Size} & \textbf{Weight} & \textbf{Frequency} \\ \hline

\small{Don't you hate it when Gandalf marks [...] (Page)} & 110 & 1.627 $\pm$ 0.001 & 19 \\ \hline
\small{Goodberry's (Page)} & 318 & 1.591 $\pm$ 0 & 73 \\ \hline
\small{Worst. Idea. Ever. [pause] Let's do it. (Page)} & 227 & 1.561 $\pm$ 0 & 21 \\ \hline
\small{CatDog (Page)} & 259 & 1.531 $\pm$ 0.001 & 12 \\ \hline
\small{Planking Australia (Page)} & 166 & 1.501 $\pm$ 0.001 & 4 \\ \hline
\small{Avenged Sevenfold (Page)} & 351 & 1.471 $\pm$ 0 & 6 \\ \hline
\small{Grug (Page)} & 279 & 1.465 $\pm$ 0 & 9 \\ \hline
\small{Dr. House (Page)} & 964 & 1.451 $\pm$ 0 & 28 \\ \hline
\small{If 1m people join, girlfriend will let [...] (Group)} & 416 & 1.362 $\pm$ 0 & 68 \\ \hline
\small{Do you ride kangaroos? no mate the [...] (Page)} & 321 & 1.333 $\pm$ 0.001 & 23 \\ \hline
  \end{tabular}
  \caption{\emph{Logistic Regression} feature weights extracted for the positive case where $k=0$. The \emph{Name} column displays the name of the feature.
                        \emph{Size} represents the size of the \emph{Page} or \emph{Group}.
                        \emph{Weight} represents the weight this feature vector received.  
                        \emph{Frequency} displays the number of times this feature vector was set to $1$ for a user.}
\end{minipage}
\end{table}

The negative LR weights are equally broken up into \emph{pages} and \emph{groups} as contributing highly to the prediction. The size ranges 
for both \emph{pages} and \emph{groups} varies greatly from as little as $27$ up to as high as $1552$ and there is little corelation between 
the frequency and sizes, in fact lower frequencies $~20$ contribute most to the prediction.

The positive LR weights are more focused on the predictiveness of \emph{pages} and the sizes of the \emph{pages} are much more consistent 
$~200$.

These LR weights show that both \emph{groups} and especially \emph{pages} are highly predictive of a users like preferences and particular 
sizes or frequencies do not appear to be more predictive then others. Additionally under this combined paradigm, \emph{favourites} do not appear 
to contribute strongly to our results.

We have shown in this chapter that a \emph{combined} version of our individually predictive affinity features results in the most 
accurate predictions for our data set, when compared with all other individual features.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
