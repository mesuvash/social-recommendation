%%
%% Template conclusion.tex
%%

\chapter{Feature Combinations}
\label{cha:bma}

As outlined above, features which positively improved classification were from the \emph{Traits}, \emph{Groups} and \emph{Pages} feature 
vectors. In this section we combine these positive feature vectors together into a larger feature vector comprised of the individual 
positively contributing elements.

\section{Positive Feature Combination}
\label{sec:notation}

Using the combined feature vector $X$ where $I$ is comprised of:
\begin{itemize}
\item \emph{Traits}
\item \emph{Groups}
\item \emph{Pages}
\end{itemize}

\clearpage

Applying this feature vector to the data set:

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/bar_combination.pdf}
		\caption{Accuracy results using the \emph{Positively Combined} feature set.}
	\end{center}
\end{figure}

\clearpage

We find that the \emph{Combination} feature vector gives better results for our classifiers when compared with our baselines. This 
holds for all values of $k$ and offers the most predictive feature vector found during this research.

This can be summarised in the table below:

\begin{table}[h]
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		SMB & 0.604 $\pm$ 0.006 \\ \hline
		NB & 0.583 $\pm$ 0.012 \\ \hline
		LR & 0.617 $\pm$ 0.01 \\ \hline
		SVM & 0.614 $\pm$ 0.009 \\ \hline
  \end{tabular}
  \caption{\emph{Traits} results for $k=0$}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		SMB & 0.604 $\pm$ 0.006 \\ \hline
		NB & 0.604 $\pm$ 0.01 \\ \hline
		LR & 0.613 $\pm$ 0.01 \\ \hline
		SVM & 0.611 $\pm$ 0.008 \\ \hline
  \end{tabular}
  \caption{\emph{Pages} results for $k=0$}
\end{minipage}
\\
\\
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		SMB & 0.604 $\pm$ 0.006 \\ \hline
		NB & 0.565 $\pm$ 0.013 \\ \hline
		LR & 0.622 $\pm$ 0.008 \\ \hline
		SVM & 0.609 $\pm$ 0.011 \\ \hline
  \end{tabular}
  \caption{\emph{Groups} results for $k=0$}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		SMB & 0.604 $\pm$ 0.006 \\ \hline
		NB & 0.605 $\pm$ 0.01 \\ \hline
		LR & 0.624 $\pm$ 0.009 \\ \hline
		SVM & 0.618 $\pm$ 0.01 \\ \hline
  \end{tabular}
  \caption{\emph{Combined} results for $k=0$}
\end{minipage}
\end{table}

These tables show that based on our results the most predictive feature vector is a combination of the individual best 
feature vectors found in our \emph{User Preferences} section.

\clearpage

Applying this same feature vector across our exposure curve:

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/line_combination.pdf}
		\caption{Accuracy results for an exposure curve using the \emph{Positively Combined} feature set.}
	\end{center}
\end{figure}

This trend continues over the exposure curve with LR, SVM and NB all improving as $k$ increases. Again, this feature vector 
combination provides the most predictive results when compared with all other analysis completed during this thesis.

By extracting the model weights from the case where $k=0$ we can see which components of the \emph{Combination} feature vector were most
predictive:

PAGES HERE

This shows us that ...

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
