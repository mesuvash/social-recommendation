%%
%% Template conclusion.tex
%%

\chapter{Feature Combinations}
\label{cha:bma}

most predictive features allows for feature selection which is crucial with limited data, cant afford to combing all.

As outlined above, features which positively improved classification were from the \emph{Traits}, \emph{Groups} and \emph{Pages} feature 
vectors. In this section we combine these positive feature vectors together into a larger feature vector comprised of the individual 
positively contributing elements.

\section{Feature Set Selection}
\label{sec:notation}

given so many features and based on size feature selection is good
time consuming and costly

Using the combined feature vector $X$ where $I$ is comprised of:
\begin{itemize}
\item \emph{Traits}
\item \emph{Groups}
\item \emph{Pages}
\end{itemize}

\clearpage

Applying this feature vector to the data set:

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/bar_combination.pdf}
		\caption{Accuracy results using the \emph{Positively Combined} feature set.}
	\end{center}
\end{figure}

\clearpage

We find that the \emph{Combination} feature vector gives better results for our classifiers when compared with our baselines. This 
holds for all values of $k$ and offers the most predictive feature vector found during this research.

This can be summarised in the table below:

\begin{table}[h]
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.583 $\pm$ 0.012 \\ \hline
		LR & 0.617 $\pm$ 0.01 \\ \hline
		SVM & 0.614 $\pm$ 0.009 \\ \hline
  \end{tabular}
  \caption{\emph{Traits} results for $k=0$}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.604 $\pm$ 0.01 \\ \hline
		LR & 0.613 $\pm$ 0.01 \\ \hline
		SVM & 0.611 $\pm$ 0.008 \\ \hline
  \end{tabular}
  \caption{\emph{Pages} results for $k=0$}
\end{minipage}
\\
\\
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.565 $\pm$ 0.013 \\ \hline
		LR & 0.622 $\pm$ 0.008 \\ \hline
		SVM & 0.609 $\pm$ 0.011 \\ \hline
  \end{tabular}
  \caption{\emph{Groups} results for $k=0$}
\end{minipage}
\begin{minipage}[b]{.50\textwidth}
\centering
  \begin{tabular}{|l|l|} % cols: (left, center, right)
  \hline
  		\textbf{Classifier} & \textbf{Accuracy} \\ \hline
		NB & 0.605 $\pm$ 0.01 \\ \hline
		LR & 0.624 $\pm$ 0.009 \\ \hline
		SVM & 0.618 $\pm$ 0.01 \\ \hline
  \end{tabular}
  \caption{\emph{Combined} results for $k=0$}
\end{minipage}
\end{table}

These tables show that based on our results the most predictive feature vector is a combination of the individual best 
feature vectors found in our \emph{User Preferences} section.

too costly to combine all features so need a subset.

\clearpage

Applying this same feature vector across our exposure curve:

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.75]{results/combination/line_combination.pdf}
		\caption{Accuracy results for an exposure curve using the \emph{Positively Combined} feature set.}
	\end{center}
\end{figure}

This trend continues over the exposure curve with LR, SVM and NB all improving as $k$ increases. Again, this feature vector 
combination provides the most predictive results when compared with all other analysis completed during this thesis.

\clearpage

By extracting the model weights from the case where $k=0$ we can see which components of the \emph{Combination} feature vector were most
predictive:
\begin{table}[h]
\begin{minipage}[b]{1.0\textwidth}
\centering
  \begin{tabular}{|l|l|l|l|l|} % cols: (left, center, right)
  \hline
  \textbf{Name} & \textbf{Size} & \textbf{Weight} & \textbf{Frequency} \\ \hline

Avatar: The Last Airbender (Page) & 324 & -1.68 $\pm$ 0.001 & 13 \\ \hline
I'm late. Got attacked by a wild Pokemon (Page) & 161 & -1.609 $\pm$ 0 & 20 \\ \hline
Overheard at the Ateneo de Manila University (Group) & 253 & -1.527 $\pm$ 0.001 & 26 \\ \hline
Sorry mate i can't, i've got Quidditch (Page) & 254 & -1.501 $\pm$ 0 & 18 \\ \hline
I would.........for Escapium. (Group) & 50 & -1.467 $\pm$ 0.001 & 11 \\ \hline
Burgtoons (Group) & 34 & -1.37 $\pm$ 0.001 & 7 \\ \hline
The Simpsons (Page) & 1552 & -1.355 $\pm$ 0.001 & 170 \\ \hline
City Gate Hall (Group) & 27 & -1.346 $\pm$ 0 & 5 \\ \hline
Victoria's Secret (Page) & 764 & -1.337 $\pm$ 0 & 11 \\ \hline
Starbucks (Page) & 1548 & -1.313 $\pm$ 0 & 7 \\ \hline
  \end{tabular}
  \caption{\emph{Logistic Regression} feature weights extracted for the negative case where $k=0$. The \emph{Name} column displays the name of the feature vector.
                        \emph{Size} represents the size of the \emph{Page} or \emph{Group}.
                        \emph{Weight} represents the weight this feature vector received.  
                        \emph{Yes'} column displays the number of times this feature vector was set to $1$ for a user.
                        \emph{Distinct} column displays the number of unique times the feature vector was set to $1$.}
\end{minipage}
\end{table}

The Negative LR weights are equally broken up into \emph{Pages} and \emph{Groups}, and implies that smaller sizes for both the \emph{Pages}
and \emph{Groups} are more predictive, however the number of distinct users who like these \emph{Groups} or \emph{Pages} are quite small.

\begin{table}[h]
\begin{minipage}[b]{1.0\textwidth}
\centering
  \begin{tabular}{|l|l|l|l|l|} % cols: (left, center, right)
  \hline
  \textbf{Name} & \textbf{Size} & \textbf{Weight} & \textbf{Distinct} \\ \hline

Don't you hate it when Gandalf marks your exam and [...] (Page) & 110 & 1.627 $\pm$ 0.001 & 19 \\ \hline
Goodberry's (Page) & 318 & 1.591 $\pm$ 0 & 73 \\ \hline
Worst. Idea. Ever. [pause] Let's do it. (Page) & 227 & 1.561 $\pm$ 0 & 21 \\ \hline
CatDog (Page) & 259 & 1.531 $\pm$ 0.001 & 12 \\ \hline
Planking Australia (Page) & 166 & 1.501 $\pm$ 0.001 & 4 \\ \hline
Avenged Sevenfold (Page) & 351 & 1.471 $\pm$ 0 & 6 \\ \hline
Grug (Page) & 279 & 1.465 $\pm$ 0 & 9 \\ \hline
Dr. House (Page) & 964 & 1.451 $\pm$ 0 & 28 \\ \hline
If 1m people join, girlfriend will let me [...] (Group) & 416 & 1.362 $\pm$ 0 & 68 \\ \hline
Do you ride kangaroos? no mate the cool kids ride emus (Page) & 321 & 1.333 $\pm$ 0.001 & 23 \\ \hline
  \end{tabular}
  \caption{\emph{Logistic Regression} feature weights extracted for the positive case where $k=0$. The \emph{Name} column displays the name of the feature vector.
                        \emph{Size} represents the size of the \emph{Page} or \emph{Group}.
                        \emph{Weight} represents the weight this feature vector received.  
                        \emph{Yes'} column displays the number of times this feature vector was set to $1$ for a user.
                        \emph{Distinct} column displays the number of unique times the feature vector was set to $1$.}
\end{minipage}
\end{table}

The Positive LR weights are equally broken up into \emph{Pages} and \emph{Groups}, though with a stronger \emph{Page} focus which and again 
implies that smaller sizes for the \emph{Pages} are more predictive, though the trend follows that the number of distinct users who like these 
\emph{Pages} are quite small.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
